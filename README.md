# Xapien Preprocessor Task

This Python package was developed as part of a technical task for **Xapien**. It provides utilities for preprocessing Markdown documents, including cleaning links, filtering overly long documents, and batching them intelligently for downstream use.

---

## âœ¨ Features

- ğŸ”— **Cleans and anonymizes external URLs**  
  Converts `[text](url)` and raw `http(s)` links to `[text](...)` or `...`

- ğŸ§® **Estimates token count**  
  Uses a word-count heuristic: 1 word â‰ˆ 1 token

- ğŸ§¹ **Filters excessively long documents**  
  Removes documents over 100,000 tokens

- ğŸ“¦ **Batches documents**  
  Uses a greedy bin-packing algorithm to group docs into batches within token and size limits

---


## ğŸ“ Project Structure

xapien_task/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ init.py # Package initializer
â”‚ â”œâ”€â”€ cleaner.py # Cleans and anonymizes URLs in markdown
â”‚ â”œâ”€â”€ filter.py # Token estimation and length filtering
â”‚ â””â”€â”€ batcher.py # Batching with greedy bin-packing
â”‚
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_cleaner.py # Unit tests for cleaner
â”‚ â”œâ”€â”€ test_filter.py # Unit tests for filter
â”‚ â””â”€â”€ test_batcher.py # Unit tests for batcher
â”‚
â”œâ”€â”€ README.md # Project documentation
â”œâ”€â”€ requirements.txt # Dependency list (if needed)
â””â”€â”€ pyproject.toml # Package metadata and config

---

## ğŸš€ Installation & Usage

Clone the repo:

```bash
git clone https://github.com/esemsc-fwd24/xapien_task.git
cd xapien_task


Install pytest for running tests:
```bash
pip install pytest
```

Example usage:
```python
from src.cleaner import clean_markdown
from src.filter import estimate_tokens, filter_documents
from src.batcher import batch_documents

text = "Visit [Xapien](https://xapien.com) and https://example.com"
cleaned = clean_markdown(text)

tokens = estimate_tokens(cleaned)

docs = [cleaned, "word " * 99_000, "word " * 101_000]
filtered = filter_documents(docs)

batches = batch_documents(filtered)
```

Running tests:
```bash
pytest tests/
```


## Assumptions
- One word â‰ˆ one token (for fast filtering without external NLP libraries)
- Token/document constraints    
    - Max 100,000 tokens per document
    - Max 10 documents or 100,000 tokens per batch
- Markdown cleaning uses regular expressions for [text](url) and raw http(s) links
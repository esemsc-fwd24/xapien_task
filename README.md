# Xapien Preprocessor Task

This Python package was developed as part of a technical task for **Xapien**. It provides utilities for preprocessing Markdown documents, including cleaning links, filtering overly long documents, and batching them intelligently for downstream use.

---

## ‚ú® Features

- üîó **Cleans and anonymizes external URLs**  
  Converts `[text](url)` and raw `http(s)` links to `[text](...)` or `...`

- üßÆ **Estimates token count**  
  Uses a word-count heuristic: 1 word ‚âà 1 token

- üßπ **Filters excessively long documents**  
  Removes documents over 100,000 tokens

- üì¶ **Batches documents**  
  Uses a greedy bin-packing algorithm to group docs into batches within token and size limits

---


## üìÅ Project Structure
```
xapien_task/
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ __init__.py 
‚îÇ ‚îú‚îÄ‚îÄ cleaner.py 
‚îÇ ‚îú‚îÄ‚îÄ filter.py 
‚îÇ ‚îú‚îÄ‚îÄ batcher.py 
‚îÇ ‚îî‚îÄ‚îÄ pipeline.py 
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ ‚îú‚îÄ‚îÄ test_cleaner.py 
‚îÇ ‚îú‚îÄ‚îÄ test_filter.py 
‚îÇ ‚îî‚îÄ‚îÄ test_batcher.py 
‚îÇ
‚îú‚îÄ‚îÄ README.md 
‚îú‚îÄ‚îÄ requirements.txt  
‚îî‚îÄ‚îÄ pyproject.toml 
```
---

> ‚ö†Ô∏è Note: This package uses a `src/` layout. To run scripts manually (e.g., `python -c`), use `from src.module import ...`.

## üöÄ Installation & Usage

To install the package lcoally:
```bash
pip install .
```

Clone the repo:

```bash
git clone https://github.com/esemsc-fwd24/xapien_task.git
cd xapien_task
```

Install pytest for running tests:
```bash
pip install pytest
```

Example usage:
```python
from src.cleaner import clean_markdown
from src.filter import estimate_tokens, filter_documents
from src.batcher import batch_documents

text = "Visit [Xapien](https://xapien.com) and https://example.com"
cleaned = clean_markdown(text)

tokens = estimate_tokens(cleaned)

docs = [cleaned, "word " * 99_000, "word " * 101_000]
filtered = filter_documents(docs)

batches = batch_documents(filtered)
```

Running tests:
```bash
pytest tests/
```


## Assumptions
- One word ‚âà one token (for fast filtering without external NLP libraries)
- Token/document constraints    
    - Max 100,000 tokens per document
    - Max 10 documents or 100,000 tokens per batch
- Markdown cleaning uses regular expressions for [text](url) and raw http(s) links